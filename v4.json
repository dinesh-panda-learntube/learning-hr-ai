{
  "path_title": "Prompt Engineering Mastery for HR Managers",
  "simulations": [
    {
      "simulation_metadata": {
        "simulation_id": "SIM_01",
        "simulation_title": "Crafting AI-Powered Job Description Prompts",
        "why_this_matters_for_getting_hired": "HR Managers who master prompt engineering can create compelling, unbiased job descriptions 10x faster.",
        "estimated_time": "15 minutes",
        "deliverables_unlockable": {
          "certifications": [
            "Job Description Prompt Master"
          ],
          "badges": [
            "AI-Enhanced JD Creation"
          ],
          "recruiter_readable_proof": [
            "Optimized Job Description Prompts",
            "Bias Detection Strategy",
            "Multi-variant JD Examples"
          ]
        },
        "primary_role_competencies_evaluated": [
          "Prompt clarity and precision",
          "Bias mitigation in AI outputs",
          "Context setting for AI models",
          "Iterative prompt refinement",
          "Output quality assessment"
        ],
        "impact_metrics": {
          "time_saved": "4 days",
          "cost_saved": "$300"
        }
      },
      "scenario_breakdown": [
        {
          "scenario_id": "S1",
          "scenario_title": "Creating Your First AI Job Description",
          "workplace_context": {
            "company_type": "Fast-growing tech startup",
            "business_state": "Need to fill 5 roles this month"
          },
          "crisis_or_decision_trigger": "Hiring manager needs a Senior Data Scientist JD by end of day.",
          "central_artefact": "Prompt template for job description generation",
          "emotional_peak": "AI generates biased or irrelevant content"
        },
        {
          "scenario_id": "S2",
          "scenario_title": "Refining Prompts for Better Results",
          "workplace_context": {
            "company_type": "Enterprise SaaS company",
            "business_state": "Strict DEI requirements"
          },
          "crisis_or_decision_trigger": "Initial AI output contains gendered language and unrealistic requirements.",
          "central_artefact": "Prompt refinement checklist",
          "emotional_peak": "Leadership questions AI tool value"
        }
      ],
      "step_level_design": [
        {
          "step_id": 1,
          "scenario_id": "S1",
          "theory_content": {
            "title": "Context Injection Strategy",
            "key_points": [
              "**Context Injection** basically means we stop asking the AI to guess stuff. It's a method used widely in top-tier tech companies because it stops the AI from lying, or 'hallucinating', when it doesn't have the facts.",
              "**Common Mistake:** When AI invents details (e.g., 'ping pong tables') because it lacks context.",
              "**Best Practice:** Never say 'Describe our culture'. Instead, paste your 'Values' page."
            ]
          },
          "instruction_question": "The prompt asks AI to 'describe' culture without data, leading to **Hallucination**. Identify the error.",
          "explain_this_question": "Correct. This forces **Hallucination**. Without injected context (values/mission), AI invents generic tropes.",
          "artefact_interaction_description": "You spot the hallucination trigger; learning to provide context instead.",
          "interaction_type": "clickable_prompt",
          "scenario_context": "The draft JD talks about 'ping pong tables' and 'fast-paced environments' which doesn't match your remote, thoughtful culture. Why?",
          "prompt_text": "Act as an expert recruiter. Write a comprehensive job description for a Senior Data Scientist role at {{1}}our Series B fintech startup{{/1}}. Use a professional but engaging tone. {{2}}Include sections for: Introduction, Responsibilities (Python, SQL, AWS){{/2}}. {{0}}Describe our unique and amazing company culture in detail{{/0}}.",
          "clickable_options": [
            "Describe our unique and amazing company culture in detail",
            "our Series B fintech startup",
            "Include sections for: Introduction, Responsibilities (Python, SQL, AWS)"
          ],
          "outcomes": {
            "correct": "Correct. This forces **Hallucination**. Without injected context (values/mission), AI invents generic tropes.",
            "partially_correct": "Context is good. The error is the instruction requiring invention.",
            "incorrect": "Structure is standard. Look for the command requiring knowledge AI doesn't have."
          },
          "immediate_feedback": "Without data, AI hallucinates. Inject context first.",
          "skill_signals_observed": [
            "Prompt precision",
            "Subjectivity detection"
          ]
        },
        {
          "step_id": 2,
          "scenario_id": "S1",
          "theory_content": {
            "title": "Constraint-Based Prompting",
            "key_points": [
              "**Constraint-Based Prompting** is all about setting boundaries. We use this in the industry because large language models are trained on the entire internet\u2014which means they've learned every human stereotype and bias.",
              "**Common Mistake:** Stereotypes hidden in the AI's training data (e.g., Engineer = Male).",
              "**Best Practice:** Use negative constraints: 'Do not use gender-coded words'."
            ]
          },
          "instruction_question": "To prevent **Latent Bias**, add a constraint to this prompt. Choose the elite syntax.",
          "explain_this_question": "Correct. Explicit constraints ('Avoid X') are stronger than vague requests ('Be professional').",
          "artefact_interaction_description": "You add a constraint to your prompt; the language tone shifts.",
          "interaction_type": "fill_blank",
          "options_inputs": [
            "Add: 'Use professional, inclusive language. Avoid jargon like rockstar, ninja, or guru. Focus on concrete skills and behaviors.'",
            "Manually edit the output after generation",
            "Accept the output as-is to save time",
            "Add: 'Make it sound cool and exciting'"
          ],
          "outcomes": {
            "correct": "Correct. Explicit constraints ('Avoid X') are stronger than vague requests ('Be professional').",
            "partially_correct": "Too vague. AI needs specific constraints.",
            "incorrect": "Undefined instruction."
          },
          "immediate_feedback": "Unconstrained prompts leak bias. Be explicit.",
          "skill_signals_observed": [
            "Bias mitigation",
            "Prompt constraints"
          ],
          "prompt_template": "Write a JD for a Developer. [____] like 'rockstar' or 'ninja'. Focus on skills.",
          "blank_options": [
            "Explicitly avoid gender-coded terms",
            "Try not to use cool words",
            "Make it sound professional",
            "Don't be biased"
          ],
          "correct_answer_index": 0
        },
        {
          "step_id": 3,
          "scenario_id": "S1",
          "theory_content": {
            "title": "Outcome-Focus vs Credentialism",
            "key_points": [
              "**Outcome-Focus** is a shift away from old-school hiring. We focus on what candidates can *do* rather than where they studied, because elite degrees don't always equal elite performance.",
              "**Common Mistake:** Over-reliance on degrees/titles as proxies for skill (creates False Negatives).",
              "**Best Practice:** Prompt for 'Demonstrated ability to [X]' not 'Degree in [Y]'."
            ]
          },
          "instruction_question": "Avoid **Credentialism** by refining the requirements section.",
          "explain_this_question": "Correct. Focusing on outcomes reduces false negatives from non-traditional backgrounds.",
          "artefact_interaction_description": "You select a requirement strategy; the JD's accessibility changes.",
          "interaction_type": "fill_blank",
          "options_inputs": [
            "Separate 'must-have' from 'nice-to-have'. Limit must-haves to 5 core skills. Ensure requirements reflect actual job needs, not wish lists.",
            "List all possible skills and qualifications",
            "Keep requirements vague and flexible",
            "Copy requirements from competitor job postings"
          ],
          "outcomes": {
            "correct": "Correct. Focusing on outcomes reduces false negatives from non-traditional backgrounds.",
            "partially_correct": "Reinforces credentialism.",
            "incorrect": "Creates barriers for skilled talent."
          },
          "immediate_feedback": "Credentials != Competence. Prompt for outcomes.",
          "skill_signals_observed": [
            "Requirement precision",
            "Candidate experience"
          ],
          "problematic_prompt": "Copy requirements from competitor job postings",
          "available_principles": [
            "Separate Must-Haves from Nice-to-Haves: Limit essentials to 5-7 items maximum",
            "Avoid Credential Inflation: Requirements should reflect actual job needs, not wish lists",
            "Use Clear Criteria: Define what proficiency or experience actually means",
            "Consider Alternate Paths: Acknowledge different ways to gain relevant skills"
          ],
          "violated_principle_indices": [
            2,
            3
          ],
          "prompt_template": "Requirements: [____] instead of specific degrees or years of tenure.",
          "blank_options": [
            "Focus on demonstrated projects and outcomes",
            "Ask for PhDs from top schools",
            "Require 10+ years experience",
            "List every possible certification"
          ],
          "correct_answer_index": 0
        },
        {
          "step_id": 4,
          "scenario_id": "S2",
          "theory_content": {
            "title": "Zero-Shot Neutrality",
            "key_points": [
              "**Zero-Shot Neutrality** is a technique to fix bias before the AI even starts writing. It's cheaper and faster than editing later, effectively cutting off bias at the source.",
              "**Common Mistake:** Language that unconsciously signals a specific gender (e.g., 'He', 'Aggressive').",
              "**Best Practice:** Force neutrality: 'Refer to the applicant as The Candidate'."
            ]
          },
          "instruction_question": "This prompt introduces **Gender Coding**. Identify the biased instruction.",
          "explain_this_question": "Correct. Using 'He' primes the AI to generate masculine text. Use 'The Candidate'.",
          "artefact_interaction_description": "You identify the pronoun bias; AI regenerates with inclusive language.",
          "interaction_type": "clickable_prompt",
          "scenario_context": "The hiring manager wrote this draft, but it seems to assume the candidate will be male.",
          "prompt_text": "Draft a requirement list for a {{1}}Lead Developer{{/1}}. {{0}}He needs to be a strong leader{{/0}} who can manage the team. Focus on {{2}}React and Node.js skills{{/2}}.",
          "clickable_options": [
            "He needs to be a strong leader",
            "Lead Developer",
            "React and Node.js skills"
          ],
          "outcomes": {
            "correct": "Correct. Using 'He' primes the AI to generate masculine text. Use 'The Candidate'.",
            "partially_correct": "Title is neutral.",
            "incorrect": "Skills are neutral."
          },
          "immediate_feedback": "Pronouns in prompts dictate output gender.",
          "skill_signals_observed": [
            "Bias prevention",
            "Systematic fixing"
          ]
        },
        {
          "step_id": 5,
          "scenario_id": "S2",
          "theory_content": {
            "title": "A/B Prompt Testing",
            "key_points": [
              "**A/B Testing** isn't just for marketing\u2014it's for prompts too. We use it to stop guessing and start measuring, ensuring that our recruiting messages actually land with the best talent.",
              "**Common Mistake:** The single element you change between versions to measure effect.",
              "**Best Practice:** Generate 3 variants changing ONLY the Tone or Emphasis."
            ]
          },
          "instruction_question": "We want to test which tone attracts more applicants. Choose the correct **Variable Parameter** setup.",
          "explain_this_question": "Correct. Testing Tone (Variable) while keeping content constant validates the impact.",
          "artefact_interaction_description": "You craft a variation prompt; 3 different JDs are generated.",
          "interaction_type": "fill_blank",
          "options_inputs": [
            "Generate 3 versions: one emphasizing growth opportunities, one highlighting technical challenges, one focused on team culture. Keep core requirements consistent.",
            "Ask AI to make random changes to the original",
            "Copy the same JD three times with minor edits",
            "Create one perfect JD instead of variants"
          ],
          "outcomes": {
            "correct": "Correct. Testing Tone (Variable) while keeping content constant validates the impact.",
            "partially_correct": "Changing salary invalidates the content test.",
            "incorrect": "Too chaotic to measure."
          },
          "immediate_feedback": "Test one variable at a time.",
          "skill_signals_observed": [
            "Strategic prompting",
            "Testing mindset"
          ],
          "prompt_template": "Generate 3 JD variants. Keep skills constant. Vary the [____] to test appeal.",
          "blank_options": [
            "Tone: Narrative vs List vs Challenge",
            "Salary range randomly",
            "Job Title entirely",
            "Company Name"
          ],
          "correct_answer_index": 0
        }
      ],
      "end_state": "You've mastered job description prompt engineering\u2014next, apply these skills to resume screening.",
      "hook_to_next_simulation": "Now you need to screen 200 resumes using AI-powered prompts."
    },
    {
      "simulation_metadata": {
        "simulation_id": "SIM_02",
        "simulation_title": "AI-Powered Resume Screening Prompts",
        "why_this_matters_for_getting_hired": "HR Managers who can engineer effective resume screening prompts reduce review time by 80% while improving quality.",
        "estimated_time": "15 minutes",
        "deliverables_unlockable": {
          "certifications": [
            "Resume Screening Prompt Expert"
          ],
          "badges": [
            "AI Screening Master"
          ],
          "recruiter_readable_proof": [
            "Screening Criteria Prompts",
            "Fair Evaluation Framework",
            "Quality Shortlist Results"
          ]
        },
        "primary_role_competencies_evaluated": [
          "Criteria definition in prompts",
          "Fair evaluation prompt design",
          "Consistency in AI screening",
          "Bias detection in results",
          "Candidate experience awareness"
        ],
        "impact_metrics": {
          "time_saved": "2 weeks",
          "cost_saved": "$800"
        }
      },
      "scenario_breakdown": [
        {
          "scenario_id": "S1",
          "scenario_title": "Setting Up Screening Criteria",
          "workplace_context": {
            "company_type": "Scale-up with high volume hiring",
            "business_state": "200 applications for 3 open roles"
          },
          "crisis_or_decision_trigger": "Hiring manager wants initial shortlist by tomorrow morning.",
          "central_artefact": "AI screening prompt template",
          "emotional_peak": "AI misses a strong candidate or flags weak ones"
        }
      ],
      "step_level_design": [
        {
          "step_id": 1,
          "scenario_id": "S1",
          "theory_content": {
            "title": "Structured Data Extraction",
            "key_points": [
              "**Structured Data Extraction** turns messy chat into usable data. In HR, this is critical because comparing paragraphs of text is impossible, but comparing rows in a table is instant.",
              "**Common Mistake:** Free text (paragraphs) that is hard to compare or filter.",
              "**Best Practice:** Force output format: 'Output as JSON' or 'Output as CSV table'."
            ]
          },
          "instruction_question": "This prompt invites **Unstructured Data**. Identify the weak instruction.",
          "explain_this_question": "Correct. 'Thoughts' results in essays. Ask for 'Score (1-10)' or 'Table'.",
          "artefact_interaction_description": "You spot the conversational drift; AI corrects to structured data.",
          "interaction_type": "clickable_prompt",
          "scenario_context": "You need a spreadsheet-ready shortlist, but this prompt is generating long essay-style reviews for each candidate.",
          "prompt_text": "Review this resume for {{1}}Product Management fit{{/1}}. {{2}}Check specifically for agile experience{{/2}}. {{0}}Let me know your thoughts{{/0}} on the candidate.",
          "clickable_options": [
            "Let me know your thoughts",
            "Product Management fit",
            "Check specifically for agile experience"
          ],
          "outcomes": {
            "correct": "Correct. 'Thoughts' results in essays. Ask for 'Score (1-10)' or 'Table'.",
            "partially_correct": "Role is clear.",
            "incorrect": "Criteria is clear."
          },
          "immediate_feedback": "Don't ask for thoughts; ask for Data.",
          "skill_signals_observed": [
            "Structured output prompting",
            "Instruction clarity"
          ]
        },
        {
          "step_id": 2,
          "scenario_id": "S1",
          "theory_content": {
            "title": "Skills-First Filtering",
            "key_points": [
              "**Skills-First Filtering** is a way to look past the resume timeline. Industry leaders use this to find hidden gems who might have employment gaps but possess world-class technical skills.",
              "**Common Mistake:** The unconscious bias to reject candidates with employment breaks.",
              "**Best Practice:** Instruct AI: 'Ignore employment dates; score based on project complexity'."
            ]
          },
          "instruction_question": "This instruction enforces a **Gap Penalty**. Identify the bias.",
          "explain_this_question": "Correct. Gaps don't equal incompetence. Filter by skill, not timeline.",
          "artefact_interaction_description": "You remove the unfair filter; the candidate pool expands.",
          "interaction_type": "clickable_prompt",
          "scenario_context": "The hiring manager wants to save time by auto-filtering, but this rule might exclude top talent.",
          "prompt_text": "Screen these resumes for the {{1}}Account Manager role{{/1}}. {{0}}Reject anyone with gaps over 6 months{{/0}}. Focus on {{2}}B2B sales experience{{/2}}.",
          "clickable_options": [
            "Reject anyone with gaps over 6 months",
            "Account Manager role",
            "B2B sales experience"
          ],
          "outcomes": {
            "correct": "Correct. Gaps don't equal incompetence. Filter by skill, not timeline.",
            "partially_correct": "Role is fine.",
            "incorrect": "Skill requirement is valid."
          },
          "immediate_feedback": "Filter by 'What', not 'When'.",
          "skill_signals_observed": [
            "Fair prompting",
            "Bias awareness"
          ]
        },
        {
          "step_id": 3,
          "scenario_id": "S1",
          "theory_content": {
            "title": "Transferable Skill Mapping",
            "key_points": [
              "**Transferable Skill Mapping** helps us hire for potential. It's used heavily because the best talent often comes from non-traditional backgrounds, like a teacher becoming a great customer success manager.",
              "**Common Mistake:** Careers that switch domains (e.g., Teacher -> Trainer -> HR).",
              "**Best Practice:** Prompt AI to map 'Adjacent Skills' (Server -> Customer Success)."
            ]
          },
          "instruction_question": "Capture **Non-Linear Paths** by refining the evaluation criteria.",
          "explain_this_question": "Correct. This captures high-potential talent from other industries.",
          "artefact_interaction_description": "You refine criteria; AI's candidate pool diversity shifts.",
          "interaction_type": "fill_blank",
          "options_inputs": [
            "Evaluate based on demonstrated skills and outcomes, not just traditional credentials. Consider bootcamp, self-taught, career transitions, and non-linear paths as valid. Focus on impact achieved.",
            "Only consider candidates with 4-year degrees from top universities",
            "Weight all backgrounds equally regardless of relevance",
            "Separate traditional and non-traditional candidates into different categories"
          ],
          "outcomes": {
            "correct": "Correct. This captures high-potential talent from other industries.",
            "partially_correct": "Too restrictive.",
            "incorrect": "Ignores transferability."
          },
          "immediate_feedback": "Map skills, don't just match titles.",
          "skill_signals_observed": [
            "Inclusive criteria",
            "Skills-based evaluation"
          ],
          "problematic_prompt": "Separate traditional and non-traditional candidates into different categories",
          "available_principles": [
            "Value Demonstrated Skills: Look for evidence of ability, not just credentials",
            "Recognize Alternate Paths: Bootcamps, self-teaching, career transitions are valid",
            "Focus on Impact: What results did they achieve regardless of how they learned?",
            "Avoid Credential Bias: Traditional paths are not inherently better"
          ],
          "violated_principle_indices": [
            2,
            3
          ],
          "prompt_template": "Evaluate candidates based on [____] rather than exact job titles.",
          "blank_options": [
            "Transferable skills and outcomes",
            "Ivy League education",
            "Previous job titles",
            "Keywords matching exactly"
          ],
          "correct_answer_index": 0
        },
        {
          "step_id": 4,
          "scenario_id": "S1",
          "theory_content": {
            "title": "Flagging vs Rejection",
            "key_points": [
              "**Flagging vs Rejection** is about keeping humans in the loop. We use this because AI lacks nuance\u2014it might reject a genius just because they don't fit a standard box.",
              "**Common Mistake:** Incorrectly flagging a good candidate as 'bad' (or vice versa).",
              "**Best Practice:** Don't auto-reject outliers (like overqualified). 'Flag for Review' instead."
            ]
          },
          "instruction_question": "Avoid **False Positives** with overqualified candidates. Choose the right logic.",
          "explain_this_question": "Correct. Keep them in the pool but tag them for specific conversation.",
          "artefact_interaction_description": "You create evaluation logic; AI flags candidates with context.",
          "options_inputs": [
            "If candidate experience significantly exceeds requirements, note it as 'potentially overqualified' with specific reasons. Do not recommend automatic rejection\u2014flag for hiring manager discussion.",
            "Auto-reject anyone with more than 2 years over requirement",
            "Ignore experience level entirely",
            "Downgrade candidates with too much experience"
          ],
          "outcomes": {
            "correct": "Correct. Keep them in the pool but tag them for specific conversation.",
            "partially_correct": "Eliminates potential talent.",
            "incorrect": "Wastes data."
          },
          "immediate_feedback": "Flag outliers; don't delete them.",
          "skill_signals_observed": [
            "Nuanced prompting",
            "Context provision"
          ],
          "interaction_type": "fill_blank",
          "prompt_template": "If candidate exceeds requirements by 5+ years, [____].",
          "blank_options": [
            "Flag as 'Senior Potential' for manual review",
            "Auto-reject for flight risk",
            "Downgrade their score",
            "Ignore the extra experience"
          ],
          "correct_answer_index": 0
        },
        {
          "step_id": 5,
          "scenario_id": "S1",
          "theory_content": {
            "title": "Algorithmic Audit",
            "key_points": [
              "**Algorithmic Auditing** is our safety check. We do this to ensure our AI isn't secretly favoring one demographic over another, keeping us compliant and fair.",
              "**Common Mistake:** When a neutral rule unintentionally hurts one protected group more.",
              "**Best Practice:** Compare 'Pass Rate' across demographics to detect hidden bias."
            ]
          },
          "instruction_question": "We need to check for **Disparate Impact**. How should we audit the AI results?",
          "explain_this_question": "Correct. If 50% of men pass but only 10% of women, you have Disparate Impact.",
          "artefact_interaction_description": "You design a validation prompt; bias patterns become visible.",
          "interaction_type": "fill_blank",
          "options_inputs": [
            "Analyze the shortlisted vs rejected groups by: gender indicators, university tiers, years of experience, career transitions. Report any significant disparities for review.",
            "Trust AI is unbiased by default",
            "Manually review every single resume",
            "Only check if someone complains"
          ],
          "outcomes": {
            "correct": "Correct. If 50% of men pass but only 10% of women, you have Disparate Impact.",
            "partially_correct": "Irrelevant metric.",
            "incorrect": "Irrelevant metric."
          },
          "immediate_feedback": "Audit the *rates* by group.",
          "skill_signals_observed": [
            "Bias auditing",
            "Quality control"
          ],
          "prompt_template": "Analyze the shortlist variances by comparing [____] against the rejected pool.",
          "blank_options": [
            "demographic pass rates",
            "resume length",
            "file formats",
            "time submitted"
          ],
          "correct_answer_index": 0
        }
      ],
      "end_state": "You've mastered AI resume screening\u2014next, learn to generate interview assessment prompts.",
      "hook_to_next_simulation": "Top candidates are advancing. Now you need AI-powered interview evaluation."
    },
    {
      "simulation_metadata": {
        "simulation_id": "SIM_03",
        "simulation_title": "Generating Interview Assessment Prompts",
        "why_this_matters_for_getting_hired": "HR Managers using AI interview prompts create consistent, fair evaluation frameworks that improve hiring decisions.",
        "estimated_time": "15 minutes",
        "deliverables_unlockable": {
          "certifications": [
            "Interview Prompt Designer"
          ],
          "badges": [
            "Structured Interview AI Master"
          ],
          "recruiter_readable_proof": [
            "Behavioral Question Prompts",
            "Scoring Framework Prompts",
            "Interview Guide Templates"
          ]
        },
        "primary_role_competencies_evaluated": [
          "Behavioral question generation",
          "Competency-based prompting",
          "Evaluation consistency",
          "Interviewer guidance creation",
          "Fair scoring frameworks"
        ],
        "impact_metrics": {
          "time_saved": "3 days",
          "cost_saved": "$450"
        }
      },
      "scenario_breakdown": [
        {
          "scenario_id": "S1",
          "scenario_title": "Creating Behavioral Interview Questions",
          "workplace_context": {
            "company_type": "Product-focused tech company",
            "business_state": "Scaling interview process for consistency"
          },
          "crisis_or_decision_trigger": "Interviewers report inconsistent candidate evaluation and unclear questions.",
          "central_artefact": "AI-generated interview question bank",
          "emotional_peak": "Interview quality varies wildly across interviewers"
        }
      ],
      "step_level_design": [
        {
          "step_id": 1,
          "scenario_id": "S1",
          "theory_content": {
            "title": "Behavioral Event Interviewing (BEI)",
            "key_points": [
              "**Behavioral Event Interviewing (BEI)** is the gold standard for predicting performance. It works because past behavior is the only reliable predictor of future action, unlike hypothetical wishes.",
              "**Common Mistake:** Question types that allow for rehearsed or fake answers.",
              "**Best Practice:** Never ask 'What would you do?'. Ask 'Tell me about a time you did...'."
            ]
          },
          "instruction_question": "This instruction solicits hypotheticals, not **BEI**. Identify the error.",
          "explain_this_question": "Correct. 'Typical questions' usually means generic hypotheticals. Ask for 'Specific Incidents'.",
          "artefact_interaction_description": "You spot the clich\u00e9 request; AI switches to scenario-based questions.",
          "interaction_type": "clickable_prompt",
          "scenario_context": "Your interviewers are complaining that candidates are giving rehearsed answers because the AI questions are too standard.",
          "prompt_text": "Generate interview questions for {{1}}Stakeholder Management{{/1}}. {{0}}Ask typical questions about problems{{/0}}. Use {{2}}STAR format{{/2}}.",
          "clickable_options": [
            "Ask typical questions about problems",
            "Stakeholder Management",
            "Use STAR format"
          ],
          "outcomes": {
            "correct": "Correct. 'Typical questions' usually means generic hypotheticals. Ask for 'Specific Incidents'.",
            "partially_correct": "Topic is fine.",
            "incorrect": "STAR is the goal."
          },
          "immediate_feedback": "Hypotheticals = Lies. Ask for History.",
          "skill_signals_observed": [
            "Creativity prompting",
            "Clich\u00e9 avoidance"
          ]
        },
        {
          "step_id": 2,
          "scenario_id": "S1",
          "theory_content": {
            "title": "Anchored Rating Scales",
            "key_points": [
              "**Anchored Rating Scales** eliminate guesswork in scoring. We use these because 'a good answer' means something different to everyone, but 'provided 3 examples' is a hard fact.",
              "**Common Mistake:** The consistency of scores between different interviewers.",
              "**Best Practice:** Define anchors: '5 = Achieved X impact'; '3 = Attempted X'."
            ]
          },
          "instruction_question": "Improve **Inter-Rater Reliability** by defining the top score anchor.",
          "explain_this_question": "Correct. Anchors must be observable facts, not feelings.",
          "artefact_interaction_description": "You generate a rubric prompt; structured scoring guide appears.",
          "interaction_type": "fill_blank",
          "options_inputs": [
            "Create a 1-5 scoring rubric for stakeholder management responses: 1=no examples, 2=basic examples, 3=clear examples with some impact, 4=strong examples with clear impact, 5=exceptional examples with measurable business outcomes. Include behavioral indicators for each level.",
            "Make a scoring system",
            "Rate candidates good or bad",
            "Give everyone the same score to be fair"
          ],
          "outcomes": {
            "correct": "Correct. Anchors must be observable facts, not feelings.",
            "partially_correct": "Too subjective.",
            "incorrect": "Too vague."
          },
          "immediate_feedback": "Define the evidence, not the vibe.",
          "skill_signals_observed": [
            "Rubric design",
            "Evaluation consistency"
          ],
          "prompt_template": "Define Score 5 as: Candidate provides [____] of business impact.",
          "blank_options": [
            "concrete, quantitative evidence",
            "a good feeling",
            "theoretical understanding",
            "perfect answers"
          ],
          "correct_answer_index": 0
        },
        {
          "step_id": 3,
          "scenario_id": "S1",
          "theory_content": {
            "title": "Recursive Probing",
            "key_points": [
              "**Recursive Probing** is how we dig for the truth. Candidates often give polished, rehearsed answers; recursive probing forces them to reveal the messy, real details underneath.",
              "**Common Mistake:** A polished, high-level answer that hides the messy details.",
              "**Best Practice:** Prompt AI to generate: 'If answer is general, ask: What specifically did YOU do?'"
            ]
          },
          "instruction_question": "Candidates often give **Surface Responses**. Set a probe to dig deeper.",
          "explain_this_question": "Correct. The 'We' trap hides individual contribution. Probe for 'I'.",
          "artefact_interaction_description": "You design follow-up logic; interviewer guide becomes more interactive.",
          "interaction_type": "fill_blank",
          "options_inputs": [
            "For each behavioral question, generate 3 follow-up probes: one exploring outcome/impact, one exploring their specific role/actions, one exploring lessons learned or what they'd do differently.",
            "Just repeat the original question louder",
            "Ask 'why' repeatedly",
            "Don't prompt for follow-ups, let interviewers improvise"
          ],
          "outcomes": {
            "correct": "Correct. The 'We' trap hides individual contribution. Probe for 'I'.",
            "partially_correct": "Too polite.",
            "incorrect": "Vague."
          },
          "immediate_feedback": "Probe for the 'I' in 'We'.",
          "skill_signals_observed": [
            "Probing strategy",
            "Signal extraction"
          ],
          "problematic_prompt": "Don't prompt for follow-ups, let interviewers improvise",
          "available_principles": [
            "Go Deeper on Vague Answers: Ask for specifics like metrics, challenges, actual decisions made",
            "Challenge Generalities: If they say we did X, ask What specifically was YOUR role?",
            "Test Critical Thinking: Follow-ups should reveal problem-solving approach, not just results",
            "Prepare Multiple Angles: Have probes ready for outcome, their specific role, and lessons learned"
          ],
          "violated_principle_indices": [
            2,
            3
          ],
          "prompt_template": "If the candidate says 'We launched', ask [____].",
          "blank_options": [
            "What was your specific role in that launch?",
            "That sounds great!",
            "What happened next?",
            "Can you elaborate?"
          ],
          "correct_answer_index": 0
        },
        {
          "step_id": 4,
          "scenario_id": "S1",
          "theory_content": {
            "title": "Compliance Guardrails",
            "key_points": [
              "**Compliance Guardrails** keep us out of court. AI doesn't know labor laws by default, so we have to explicitly forbid it from asking illegal questions about age or family.",
              "**Common Mistake:** Groups protected by law (Age, Race, Family Status, etc.).",
              "**Best Practice:** Hard constraint: 'Do not ask about personal life, kids, or age'."
            ]
          },
          "instruction_question": "This question violates **Protected Class** laws. Identify it.",
          "explain_this_question": "Correct. Family status is a Protected Class. Asking causes liability.",
          "artefact_interaction_description": "You flag the illegal question; AI removes it for legal compliance.",
          "interaction_type": "clickable_prompt",
          "scenario_context": "The hiring manager wants to ensure the candidate can handle long hours, but this prompt goes too far.",
          "prompt_text": "Create an interview script for a {{1}}Sales Manager{{/1}}. {{0}}Ask if they are planning to have kids soon{{/0}} (for scheduling). {{2}}Check if they can travel 50% of the time{{/2}}.",
          "clickable_options": [
            "Ask if they are planning to have kids soon",
            "Sales Manager",
            "Check if they can travel 50% of the time"
          ],
          "outcomes": {
            "correct": "Correct. Family status is a Protected Class. Asking causes liability.",
            "partially_correct": "Role is fine.",
            "incorrect": "Travel is a valid job requirement."
          },
          "immediate_feedback": "Never ask about family. Focus on availability.",
          "skill_signals_observed": [
            "Fair question design",
            "Bias prevention"
          ]
        },
        {
          "step_id": 5,
          "scenario_id": "S1",
          "theory_content": {
            "title": "Structured Interview Protocol",
            "key_points": [
              "**Structured Interview Protocol** ensures that every candidate gets a fair shot. By strictly timing each section, we stop charismatic talkers from dominating the clock and focus on the skills.",
              "**Common Mistake:** Strictly allocating minutes to each section to prevent rambles.",
              "**Best Practice:** Guide: '10 min for Skill A, 10 min for Skill B'. No overruns."
            ]
          },
          "instruction_question": "Ensure fairness using **Time-Boxing**. Complete the prompt.",
          "explain_this_question": "Correct. Without time limits, charismatic candidates hog the clock.",
          "artefact_interaction_description": "You prompt for full guide; formatted document is generated.",
          "interaction_type": "fill_blank",
          "options_inputs": [
            "Create a 60-minute interview guide for Engineering Manager role: 5 min intro, 40 min behavioral questions (8 min each for 5 competencies with scoring rubrics), 10 min candidate questions, 5 min close. Include what to say at each stage.",
            "Just list some questions",
            "Copy interview guide from the internet",
            "Tell interviewers to wing it"
          ],
          "outcomes": {
            "correct": "Correct. Without time limits, charismatic candidates hog the clock.",
            "partially_correct": "Allows bias.",
            "incorrect": "Unfair."
          },
          "immediate_feedback": "Fairness = Standardized Time.",
          "skill_signals_observed": [
            "Process design",
            "Comprehensive prompting"
          ],
          "prompt_template": "Structure the guide with [____] for each competency to ensure equal opportunity.",
          "blank_options": [
            "equal, strict time limits",
            "approximate timing",
            "flexible open time",
            "as much time as needed"
          ],
          "correct_answer_index": 0
        }
      ],
      "end_state": "You've created AI-powered interview frameworks\u2014next comes employee feedback.",
      "hook_to_next_simulation": "Performance review season is here. Time to prompt AI for feedback frameworks."
    }
  ]
}